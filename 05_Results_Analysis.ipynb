{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAaGCke86lXgqztHgsgg9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theresaskruzna/riiid_knowledge_tracing/blob/main/05_Results_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxe7zvVAIT54"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(model, X_val, y_val, history=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the model with various metrics and visualizations.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained Keras model\n",
        "    X_val: Validation features\n",
        "    y_val: Validation labels\n",
        "    history: Training history object (optional)\n",
        "\n",
        "    Returns:\n",
        "    eval_results: Dictionary containing evaluation metrics and figures\n",
        "    \"\"\"\n",
        "    eval_results = {}\n",
        "\n",
        "    # 1. Get basic metrics\n",
        "    print(\"Calculating basic metrics...\")\n",
        "    y_pred_proba = model.predict(X_val)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Loss, accuracy and AUC from model.evaluate()\n",
        "    loss, accuracy, auc_score = model.evaluate(X_val, y_val, verbose=0)\n",
        "    eval_results['loss'] = loss\n",
        "    eval_results['accuracy'] = accuracy\n",
        "    eval_results['auc'] = auc_score\n",
        "\n",
        "    # 2. Confusion Matrix\n",
        "    print(\"Creating confusion matrix...\")\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    eval_results['confusion_matrix_fig'] = plt\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Classification Report\n",
        "    print(\"Generating classification report...\")\n",
        "    report = classification_report(y_val, y_pred, output_dict=True)\n",
        "    eval_results['classification_report'] = report\n",
        "\n",
        "    # 4. ROC Curve\n",
        "    print(\"Plotting ROC curve...\")\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    eval_results['roc_curve_fig'] = plt\n",
        "    plt.savefig('roc_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Precision-Recall Curve\n",
        "    print(\"Plotting precision-recall curve...\")\n",
        "    precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.3f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.tight_layout()\n",
        "    eval_results['pr_curve_fig'] = plt\n",
        "    plt.savefig('precision_recall_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 6. Training History Plots (if history is provided)\n",
        "    if history:\n",
        "        print(\"Plotting training history...\")\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Loss plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Accuracy plot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        eval_results['training_history_fig'] = plt\n",
        "        plt.savefig('training_history.png')\n",
        "        plt.close()\n",
        "\n",
        "        # AUC plot\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(history.history['auc'], label='Training AUC')\n",
        "        plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "        plt.title('Model AUC')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('AUC')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        eval_results['auc_history_fig'] = plt\n",
        "        plt.savefig('auc_history.png')\n",
        "        plt.close()\n",
        "\n",
        "    # 7. Prediction distribution\n",
        "    print(\"Plotting prediction distribution...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(y_pred_proba, bins=50, kde=True)\n",
        "    plt.axvline(x=0.5, color='r', linestyle='--')\n",
        "    plt.title('Distribution of Prediction Probabilities')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    eval_results['pred_distribution_fig'] = plt\n",
        "    plt.savefig('prediction_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 8. Model performance by user (if user IDs are available)\n",
        "    # This would require the user_ids from the validation set\n",
        "\n",
        "    # 9. Generate a summary of the evaluation\n",
        "    summary = {\n",
        "        'accuracy': accuracy,\n",
        "        'auc': auc_score,\n",
        "        'precision': report['1']['precision'],\n",
        "        'recall': report['1']['recall'],\n",
        "        'f1_score': report['1']['f1-score'],\n",
        "    }\n",
        "    eval_results['summary'] = summary\n",
        "\n",
        "    # Create a markdown report for easy presentation\n",
        "    report_md = f\"\"\"\n",
        "    # Model Evaluation Report\n",
        "\n",
        "    ## Performance Metrics\n",
        "\n",
        "    | Metric | Value |\n",
        "    |--------|-------|\n",
        "    | Accuracy | {accuracy:.4f} |\n",
        "    | AUC | {auc_score:.4f} |\n",
        "    | Precision | {report['1']['precision']:.4f} |\n",
        "    | Recall | {report['1']['recall']:.4f} |\n",
        "    | F1 Score | {report['1']['f1-score']:.4f} |\n",
        "\n",
        "    ## Key Findings\n",
        "\n",
        "    - The model achieved {accuracy*100:.2f}% accuracy on the validation set\n",
        "    - Area Under the ROC Curve (AUC) is {auc_score:.4f}\n",
        "    - The model's precision is {report['1']['precision']:.4f}, meaning that {report['1']['precision']*100:.2f}% of predicted positive answers were actually correct\n",
        "    - The model's recall is {report['1']['recall']:.4f}, meaning it correctly identified {report['1']['recall']*100:.2f}% of all correct answers\n",
        "\n",
        "    ## Visualizations\n",
        "\n",
        "    See the generated plots:\n",
        "    - Confusion Matrix\n",
        "    - ROC Curve\n",
        "    - Precision-Recall Curve\n",
        "    - Training History\n",
        "    - Prediction Distribution\n",
        "    \"\"\"\n",
        "\n",
        "    with open('model_evaluation_report.md', 'w') as f:\n",
        "        f.write(report_md)\n",
        "\n",
        "    print(\"Evaluation complete! Report saved as 'model_evaluation_report.md'\")\n",
        "    print(\"All figures have been saved as PNG files\")\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Add this to your main function after training\n",
        "def add_evaluation_to_main():\n",
        "    # After training in your main function:\n",
        "    print(\"Evaluating model...\")\n",
        "    eval_results = evaluate_model(trained_model, X_val, y_val, history)\n",
        "\n",
        "    # You can now use the eval_results dictionary for further analysis\n",
        "    # or to generate presentation materials\n",
        "\n",
        "    # Example: Print the summary metrics\n",
        "    print(\"\\nEvaluation Summary:\")\n",
        "    for metric, value in eval_results['summary'].items():\n",
        "        print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_basic_metrics(model, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Calculate basic evaluation metrics for the model.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained Keras model\n",
        "    X_val: Validation features\n",
        "    y_val: Validation labels\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing basic evaluation metrics\n",
        "    \"\"\"\n",
        "    print(\"Calculating basic metrics...\")\n",
        "    y_pred_proba = model.predict(X_val)\n",
        "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Get metrics from model.evaluate()\n",
        "    loss, accuracy, auc_score = model.evaluate(X_val, y_val, verbose=0)\n",
        "\n",
        "    # Get classification report\n",
        "    report = classification_report(y_val, y_pred, output_dict=True)\n",
        "\n",
        "    metrics = {\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy,\n",
        "        'auc': auc_score,\n",
        "        'precision': report['1']['precision'],\n",
        "        'recall': report['1']['recall'],\n",
        "        'f1_score': report['1']['f1-score'],\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC: {auc_score:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_val, y_pred, save_path=None):\n",
        "    \"\"\"\n",
        "    Create and plot confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    y_val: Validation labels\n",
        "    y_pred: Predicted labels\n",
        "    save_path: Path to save the figure (optional)\n",
        "\n",
        "    Returns:\n",
        "    plt.Figure: Matplotlib figure object\n",
        "    \"\"\"\n",
        "    print(\"Creating confusion matrix...\")\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Confusion matrix saved to {save_path}\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "\n",
        "def plot_roc_curve(y_val, y_pred_proba, save_path=None):\n",
        "    \"\"\"\n",
        "    Create and plot ROC curve.\n",
        "\n",
        "    Parameters:\n",
        "    y_val: Validation labels\n",
        "    y_pred_proba: Predicted probabilities\n",
        "    save_path: Path to save the figure (optional)\n",
        "\n",
        "    Returns:\n",
        "    plt.Figure: Matplotlib figure object\n",
        "    float: ROC AUC score\n",
        "    \"\"\"\n",
        "    print(\"Plotting ROC curve...\")\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"ROC curve saved to {save_path}\")\n",
        "\n",
        "    return plt, roc_auc\n",
        "\n",
        "\n",
        "def plot_precision_recall_curve(y_val, y_pred_proba, save_path=None):\n",
        "    \"\"\"\n",
        "    Create and plot Precision-Recall curve.\n",
        "\n",
        "    Parameters:\n",
        "    y_val: Validation labels\n",
        "    y_pred_proba: Predicted probabilities\n",
        "    save_path: Path to save the figure (optional)\n",
        "\n",
        "    Returns:\n",
        "    plt.Figure: Matplotlib figure object\n",
        "    float: PR AUC score\n",
        "    \"\"\"\n",
        "    print(\"Plotting precision-recall curve...\")\n",
        "    precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.3f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Precision-Recall curve saved to {save_path}\")\n",
        "\n",
        "    return plt, pr_auc\n",
        "\n",
        "\n",
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot training history metrics.\n",
        "\n",
        "    Parameters:\n",
        "    history: Keras history object from model.fit()\n",
        "    save_path: Path to save the figure (optional)\n",
        "\n",
        "    Returns:\n",
        "    plt.Figure: Matplotlib figure object\n",
        "    \"\"\"\n",
        "    if not history:\n",
        "        print(\"No training history provided.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Plotting training history...\")\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Training history saved to {save_path}\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "\n",
        "def plot_auc_history(history, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot AUC history.\n",
        "\n",
        "    Parameters:\n",
        "    history: Keras history object from model.fit()\n",
        "    save_path: Path to save the figure (optional)\n",
        "\n",
        "    Returns:\n",
        "    plt.Figure: Matplotlib figure object\n",
        "    \"\"\"\n",
        "    if not history or 'auc' not in history.history:\n",
        "        print(\"No AUC history available in the provided history object.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Plotting AUC history...\")\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(history.history['auc'], label='Training AUC')\n",
        "    plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "    plt.title('Model AUC')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"AUC history saved to {save_path}\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "\n",
        "def plot_prediction_distribution(y_pred_proba, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot distribution of prediction probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    y_pred_proba: Predicted probabilities\n",
        "    save_path: Path to save the figure (optional)\n",
        "\n",
        "    Returns:\n",
        "    plt.Figure: Matplotlib figure object\n",
        "    \"\"\"\n",
        "    print(\"Plotting prediction distribution...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(y_pred_proba, bins=50, kde=True)\n",
        "    plt.axvline(x=0.5, color='r', linestyle='--')\n",
        "    plt.title('Distribution of Prediction Probabilities')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Prediction distribution saved to {save_path}\")\n",
        "\n",
        "    return plt\n",
        "\n",
        "\n",
        "def generate_markdown_report(metrics, save_path='model_evaluation_report.md'):\n",
        "    \"\"\"\n",
        "    Generate a markdown report summarizing model performance.\n",
        "\n",
        "    Parameters:\n",
        "    metrics: Dictionary containing evaluation metrics\n",
        "    save_path: Path to save the markdown report\n",
        "\n",
        "    Returns:\n",
        "    str: Markdown report content\n",
        "    \"\"\"\n",
        "    print(\"Generating markdown report...\")\n",
        "\n",
        "    report_md = f\"\"\"\n",
        "    # Model Evaluation Report\n",
        "\n",
        "    ## Performance Metrics\n",
        "\n",
        "    | Metric | Value |\n",
        "    |--------|-------|\n",
        "    | Accuracy | {metrics['accuracy']:.4f} |\n",
        "    | AUC | {metrics['auc']:.4f} |\n",
        "    | Precision | {metrics['precision']:.4f} |\n",
        "    | Recall | {metrics['recall']:.4f} |\n",
        "    | F1 Score | {metrics['f1_score']:.4f} |\n",
        "\n",
        "    ## Key Findings\n",
        "\n",
        "    - The model achieved {metrics['accuracy']*100:.2f}% accuracy on the validation set\n",
        "    - Area Under the ROC Curve (AUC) is {metrics['auc']:.4f}\n",
        "    - The model's precision is {metrics['precision']:.4f}, meaning that {metrics['precision']*100:.2f}% of predicted positive answers were actually correct\n",
        "    - The model's recall is {metrics['recall']:.4f}, meaning it correctly identified {metrics['recall']*100:.2f}% of all correct answers\n",
        "\n",
        "    ## Visualizations\n",
        "\n",
        "    See the generated plots:\n",
        "    - Confusion Matrix\n",
        "    - ROC Curve\n",
        "    - Precision-Recall Curve\n",
        "    - Training History\n",
        "    - Prediction Distribution\n",
        "    \"\"\"\n",
        "\n",
        "    with open(save_path, 'w') as f:\n",
        "        f.write(report_md)\n",
        "\n",
        "    print(f\"Markdown report saved to {save_path}\")\n",
        "\n",
        "    return report_md\n",
        "\n",
        "\n",
        "# Example usage in your main file:\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example of how to use the evaluation functions in your main code.\n",
        "    \"\"\"\n",
        "    # After training your model:\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    metrics = calculate_basic_metrics(trained_model, X_val, y_val)\n",
        "\n",
        "    # Generate plots\n",
        "    plot_confusion_matrix(y_val, metrics['y_pred'], save_path='confusion_matrix.png')\n",
        "    plot_roc_curve(y_val, metrics['y_pred_proba'], save_path='roc_curve.png')\n",
        "    plot_precision_recall_curve(y_val, metrics['y_pred_proba'], save_path='precision_recall_curve.png')\n",
        "\n",
        "    # If you have training history\n",
        "    plot_training_history(history, save_path='training_history.png')\n",
        "    plot_auc_history(history, save_path='auc_history.png')\n",
        "\n",
        "    # Plot prediction distribution\n",
        "    plot_prediction_distribution(metrics['y_pred_proba'], save_path='prediction_distribution.png')\n",
        "\n",
        "    # Generate markdown report\n",
        "    generate_markdown_report(metrics)"
      ],
      "metadata": {
        "id": "-yYlEaKV1KKm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}