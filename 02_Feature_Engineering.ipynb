{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyZvRpCTfztxaJWOSjl31J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theresaskruzna/riiid_knowledge_tracing/blob/main/02_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering"
      ],
      "metadata": {
        "id": "2PFEMQrPjhAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature creation"
      ],
      "metadata": {
        "id": "PLCZrE4wjkyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding: Converting categorical variables to binary columns"
      ],
      "metadata": {
        "id": "Gxrx4cBwYrNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['part', 'type_of']  # Columns for one-hot encoding\n",
        "\n",
        "lectures_encoded = lectures_df.copy()  # Create a copy of the DataFrame\n",
        "\n",
        "# Apply one-hot encoding with dtype=int to get binary values and drop original columns\n",
        "for col in categorical_cols:\n",
        "    one_hot_encoded = pd.get_dummies(lectures_df[['part', 'type_of']], prefix=['part', 'type'])\n",
        "    lectures_encoded = pd.concat([lectures_df, one_hot_encoded], axis=1)\n",
        "    lectures_encoded.drop(columns=[col], inplace=True)  # Drop the original column\n",
        "\n",
        "lectures_encoded.head()"
      ],
      "metadata": {
        "id": "9PC_WIZYdoml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = ['correct_answer','part']  # Columns for one-hot encoding\n",
        "\n",
        "questions_encoded = questions_df.copy()  # Create a copy of the DataFrame\n",
        "\n",
        "# Apply one-hot encoding with dtype=int to get binary values and drop original columns\n",
        "for col in categorical_cols:\n",
        "    one_hot_encoded = pd.get_dummies(questions_df[['correct_answer','part']], prefix=['correct_answer','part'])\n",
        "    questions_encoded = pd.concat([questions_df, one_hot_encoded], axis=1)\n",
        "    questions_encoded.drop(columns=[col], inplace=True)  # Drop the original column\n",
        "\n",
        "questions_encoded.head()"
      ],
      "metadata": {
        "id": "CBtEav_ofdbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Progress Features\n",
        "\n",
        "Knowledge growth rate: Change in correctness over time"
      ],
      "metadata": {
        "id": "900EZ_j0ZKoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort data by user and time\n",
        "train_sorted = train.sort_values(['user_id', 'timestamp'])\n",
        "\n",
        "# Create time bins (e.g., by day, week, or by groups of N questions)\n",
        "# Option 1: By time periods\n",
        "train_sorted['time_bin'] = pd.cut(train_sorted['timestamp'], bins=10)  # 10 equal time bins\n",
        "\n",
        "# Option 2: By question sequence (every N questions)\n",
        "train_sorted['question_seq'] = train_sorted.groupby('user_id').cumcount()\n",
        "train_sorted['question_bin'] = train_sorted['question_seq'] // 10  # Every 10 questions\n",
        "\n",
        "# Calculate correctness rate by user and time bin\n",
        "growth_rate = train_sorted.loc[train_sorted.content_type_id == False,\n",
        "                               ['user_id', 'time_bin', 'answered_correctly']\n",
        "                              ].groupby(['user_id', 'time_bin']).agg(['mean'])\n",
        "\n",
        "growth_rate.columns = ['correctness_rate']\n",
        "growth_rate = growth_rate.reset_index()\n",
        "\n",
        "# Calculate the change between consecutive time periods\n",
        "growth_rate_change = growth_rate.sort_values(['user_id', 'time_bin'])\n",
        "growth_rate_change['previous_rate'] = growth_rate_change.groupby('user_id')['correctness_rate'].shift(1)\n",
        "growth_rate_change['growth_rate'] = growth_rate_change['correctness_rate'] - growth_rate_change['previous_rate']\n",
        "\n",
        "# Calculate average growth rate per user\n",
        "user_avg_growth = growth_rate_change.groupby('user_id')['growth_rate'].mean()"
      ],
      "metadata": {
        "id": "dH5x7C_3mB_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average correctness rate for each user (what percentage of questions they answered correctly)"
      ],
      "metadata": {
        "id": "RF9TnvnTlN5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\n",
        "results_u_final.columns = ['answered_correctly_user']"
      ],
      "metadata": {
        "id": "VI77sVmTj7xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " 'avg_questions', 'avg_questions_seen'"
      ],
      "metadata": {
        "id": "8aRuke0vjf2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average rate at which users received explanations for prior questions"
      ],
      "metadata": {
        "id": "OD9yj_jqlis9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
        "results_u2_final.columns = ['explanation_mean_user']"
      ],
      "metadata": {
        "id": "BxvPtS2PlYBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_u2_final.explanation_mean_user.describe()"
      ],
      "metadata": {
        "id": "WR_t-Gmkl08P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many times a unique question has been answered and what is the percentage of success for each unique question"
      ],
      "metadata": {
        "id": "wDaac9qT5YBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging datasets"
      ],
      "metadata": {
        "id": "jGEbjlmQfEwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal is to merge train, questions and lectures datasets for model building"
      ],
      "metadata": {
        "id": "MF72YDc7fK3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "question_id is a foreign key for the train/test content_id column, when the content type is question (0)"
      ],
      "metadata": {
        "id": "sFTPsBMIfRSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lecture_id is a foreign key for the train/test content_id column, when the content type is lecture (1)"
      ],
      "metadata": {
        "id": "IaVc489Kfftr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def merge_datasets(train_df, questions_df, lectures_df):\n",
        "    \"\"\"\n",
        "    Merge three datasets: train_df, questions_df, and lectures_df.\n",
        "\n",
        "    Parameters:\n",
        "    - train_df: DataFrame containing 'content_id' as the key column\n",
        "    - questions_df: DataFrame containing 'question_id' that links to 'content_id' in train_df\n",
        "    - lectures_df: DataFrame containing 'lecture_id' that links to 'content_id' in train_df\n",
        "\n",
        "    Returns:\n",
        "    - merged_df: The complete merged DataFrame\n",
        "    \"\"\"\n",
        "    # Mrge train_df with questions_df\n",
        "    # Use a left merge to keep all rows from train_df\n",
        "    questions_merged = train_df.merge(\n",
        "        questions_df,\n",
        "        left_on='content_id',\n",
        "        right_on='question_id',\n",
        "        how='left',\n",
        "        suffixes=('', '_question')\n",
        "    )\n",
        "\n",
        "    # Merge the resulting DataFrame with lectures_df\n",
        "    # Use a left merge to keep all rows from our previous merge\n",
        "    final_merged = questions_merged.merge(\n",
        "        lectures_df,\n",
        "        left_on='content_id',\n",
        "        right_on='lecture_id',\n",
        "        how='left',\n",
        "        suffixes=('', '_lecture')\n",
        "    )\n",
        "\n",
        "    # Handle the common part_1 to part_7 columns to avoid confusion\n",
        "    # For each common column, create a new column that takes the value from either source\n",
        "    for i in range(1, 8):\n",
        "        part_col = f'part_{i}'\n",
        "        question_col = f'{part_col}'  # If no suffix was added in first merge\n",
        "        lecture_col = f'{part_col}_lecture'\n",
        "\n",
        "        # Use coalesce logic: take the first non-null value\n",
        "        # If the value comes from questions_df, use it; otherwise, try lectures_df\n",
        "        final_merged[f'combined_{part_col}'] = final_merged[question_col].combine_first(\n",
        "            final_merged[lecture_col]\n",
        "        )\n",
        "\n",
        "    return final_merged\n",
        "\n",
        "# Example usage\n",
        "# merged_df = merge_datasets(train_df, questions_df, lectures_df)"
      ],
      "metadata": {
        "id": "sw0iVliOj_iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merge_datasets(train_df, questions_df, lectures_df)"
      ],
      "metadata": {
        "id": "Mw_OmYr8oTiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling"
      ],
      "metadata": {
        "id": "5YoS8pzr1GHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure same scale for all columns from 0-1\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "# convert to a dataframe, for better readability\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "X_scaled.head()"
      ],
      "metadata": {
        "id": "JdPDcdUo4TaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the numerical columns for standardization\n",
        "num_col = maths_encoded.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize the numerical columns\n",
        "maths_encoded_standardized = maths_encoded.copy()  # Create a copy to avoid modifying the original\n",
        "maths_encoded_standardized[num_col] = scaler.fit_transform(maths_encoded[num_col])\n",
        "\n",
        "# Display the first few rows of the standardized data\n",
        "maths_encoded_standardized.head()"
      ],
      "metadata": {
        "id": "S43YYyj54aIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}