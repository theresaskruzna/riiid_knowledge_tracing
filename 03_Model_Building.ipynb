{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrLHmKMy6xrmq2xP2qWuf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theresaskruzna/riiid_knowledge_tracing/blob/main/03_Model_Building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNbTad_dHiz_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Input, Dropout, Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import regularizers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building an RNN - LSTM model, binary classification"
      ],
      "metadata": {
        "id": "KSuWj6Jelvky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential model using the Keras API in TensorFlow. This model is specifically designed for sequence data processing, for tasks like time series forecasting, natural language processing, or any other task involving sequential patterns."
      ],
      "metadata": {
        "id": "j1mJmq_c97-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Model: The use of a Sequential model is not limited to image classification. You can use a Sequential model to build a network for time series forecasting by replacing the image-specific layers with layers suitable for sequential data, such as LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) layers."
      ],
      "metadata": {
        "id": "IKiL5UDE_udD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequential_lstm_model(input_shape, hidden_units=128): # hidden_units=128 is default number that sets the number of neurons\n",
        "    model = Sequential() # creates sequential model = linear stack of layers\n",
        "    # input fits on X\n",
        "    model.add(Input(shape=X_train.shape[1:]))\n",
        "    # First LSTM(i.e.long short-term memory) layer with return sequences for stacking\n",
        "    model.add(LSTM(hidden_units, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2)) # prevent overfitting - randomly ignore 20% neurons during training\n",
        "    # use recurrent_dropout=0.2?\n",
        "\n",
        "    # Second LSTM layer without return sequences for final output\n",
        "    model.add(LSTM(hidden_units, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(BatchNormalization()) # stabilize training and improve convergence\n",
        "\n",
        "    # Output layer with one neuron (for binary classification), sigmoid produces a probability between 0 and 1\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.01))) # L2 regularisor to prevent overfitting\n",
        "\n",
        "    model.compile( # configure learning process of the model\n",
        "        optimizer='adam', # specify opitimisation algorithm for training\n",
        "        loss='binary_crossentropy', # measure difference between predictions and actual values\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC()] # set metrics to track during training and evaluation, AUC(area under the curve)\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mv3cPf8fgF2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tweak model by changing number of hidden units and layers to optimise the architecture of the model"
      ],
      "metadata": {
        "id": "E5s2ebDxm9rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping"
      ],
      "metadata": {
        "id": "KfYU5mhPnbdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    # monitor accuracy of the validation dataset\n",
        "    monitor=\"val_accuracy\",\n",
        "    # if it doesn't improve by at least 0.3%\n",
        "    min_delta=0.003,\n",
        "    # within the last 10 epochs\n",
        "    patience=10,\n",
        "    # turn it off and restore weights from the epoch with the highest accuracy on the validation set\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Q07sqqL_nUbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model checkpoint"
      ],
      "metadata": {
        "id": "6kZuCSjgosV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save = ModelCheckpoint(\n",
        "    # where to save model\n",
        "    filepath=\"best_model.keras\",\n",
        "    # monitor accuracy of the validation set\n",
        "    monitor=\"val_accuracy\",\n",
        "    # save only one file with the highest metrics value\n",
        "    save_best_only=True,\n",
        "    # save architecture and wights into one file\n",
        "    save_weights_only=False,\n",
        "    # after every epoch\n",
        "    save_freq=\"epoch\"\n",
        ")"
      ],
      "metadata": {
        "id": "WtBUpP7SorN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "J1tOxfQM9Hho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=1000, validation_data=(X_test, y_test), batch_size=32, callbacks=[early, save])"
      ],
      "metadata": {
        "id": "iiaeKVvU-ojk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Příklad dat\n",
        "data = np.sin(np.linspace(0, 100, 500))  # simulovaná časová řada\n",
        "window_size = 60\n",
        "prediction_steps = 14\n",
        "\n",
        "# Vytvoření vstupů a výstupů\n",
        "X, y = [], []\n",
        "for i in range(len(data) - window_size - prediction_steps):\n",
        "    X.append(data[i:i+window_size])\n",
        "    y.append(data[i+window_size:i+window_size+prediction_steps])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "model = Sequential([\n",
        "    LSTM(64, activation='relu', input_shape=(X.shape[1], 1)),\n",
        "    Dense(prediction_steps)  # Výstupní vrstva s 14 hodnotami\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "# Úprava tvaru dat (přidání dimenze pro jednotlivé kanály)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Trénování\n",
        "model.fit(X, y, epochs=20, batch_size=32)"
      ],
      "metadata": {
        "id": "_9uKM7iZ_W5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code DOES demonstrate a basic time series forecasting setup using an LSTM layer. It focuses on predicting future values in a sequence, which is a common task in time series analysis."
      ],
      "metadata": {
        "id": "De96pYX6_5xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model - Training function with callbacks"
      ],
      "metadata": {
        "id": "GRZxA06HCEt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, batch_size=64, epochs=10):\n",
        "    # Define callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'best_sequential_model.h5',\n",
        "        monitor='val_auc',\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    )\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        patience=3,\n",
        "        mode='max',\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=0.0001\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "VmmcBjhzCETn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}